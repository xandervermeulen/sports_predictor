{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Combining the data from csv files into a single file. We want per fight: Fighter A with columns: Fighter A, with their respective data, fighter B with their respective data. The winner of the fight and the method of winning. The data will be used to train a model to predict the winner of a fight."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ad3815d6bcc7b18"
  },
  {
   "cell_type": "markdown",
   "source": [
    "imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a11cb4e72083015a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-20T21:03:46.052622Z",
     "start_time": "2024-05-20T21:03:37.801289Z"
    }
   },
   "source": [
    "import config.ConnectionConfig as cc\n",
    "import pandas as pd\n",
    "\n",
    "cc.setupEnvironment()\n",
    "spark = cc.startLocalCluster(\"UFC_Fighter_Stats\")\n",
    "spark.getActiveSession()"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T21:05:41.849468Z",
     "start_time": "2024-05-20T21:05:41.809223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fight_results = pd.read_csv('../processed_data/all_fighter_details.csv', sep=',', index_col=0)\n",
    "fight_results"
   ],
   "id": "7019429300917d63",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# read the data from the csv files\n",
    "predictions = pd.read_csv('../processed_data/upcoming_fights_predictions.csv', sep=',')\n",
    "predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T21:03:46.876725Z",
     "start_time": "2024-05-20T21:03:46.054600Z"
    }
   },
   "id": "35fa54a392094e1c",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we will calculate averages of certain data per fighter ( statistics of certain punches etc. ). We want to add this to a total fighter csv file."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "855d90967cbad2b2"
  },
  {
   "cell_type": "code",
   "source": [
    "df_operational_base_stats = spark.read.csv('../scraped_data/ufc_fighter_tott.csv', header=True)\n",
    "df_operational_to_calc_stats = spark.read.csv('../scraped_data/ufc_fight_stats.csv', header=True)\n",
    "df_operational_to_calc_result = spark.read.csv('../scraped_data/ufc_fight_results.csv', header=True)\n",
    "\n",
    "df_operational_base_stats.show()\n",
    "df_operational_to_calc_stats.show()\n",
    "df_operational_to_calc_result.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T11:36:18.639929Z",
     "start_time": "2024-05-08T11:36:15.739174Z"
    }
   },
   "id": "b096131c071db8b8",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_operational_to_calc_stats = df_operational_to_calc_stats.withColumnRenamed(\"TD %\", \"TD_Percentage\")\n",
    "df_operational_to_calc_stats = df_operational_to_calc_stats.withColumnRenamed(\"SIG.STR. %\", \"Significant_Strike_Percentage\")\n",
    "df_operational_to_calc_stats = df_operational_to_calc_stats.withColumnRenamed(\"SUB.ATT\", \"SUB_ATT\")\n",
    "\n",
    "df_operational_base_stats.createOrReplaceTempView(\"fighter_info\")\n",
    "df_operational_to_calc_stats.createOrReplaceTempView(\"fight_stats\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T11:36:18.749473Z",
     "start_time": "2024-05-08T11:36:18.641931Z"
    }
   },
   "id": "dbccd81fea57f530",
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "updated_fighter_info = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "    fighter_info.FIGHTER, fighter_info.Height, fighter_info.Weight, fighter_info.Reach, \n",
    "    AVG(fight_stats.KD) AS AVG_KD,\n",
    "    AVG(fight_stats.SUB_ATT) AS AVG_SUB_ATT,\n",
    "    AVG(regexp_replace(fight_stats.TD_Percentage, '%', '')) AS AVG_TD_Percentage,\n",
    "    AVG(regexp_replace(fight_stats.Significant_Strike_Percentage, '%', '')) AS AVG_Significant_Strike_Percentage,\n",
    "    AVG((CAST(SUBSTRING_INDEX(fight_stats.`TOTAL STR.`, ' of ', 1) AS INT) + \n",
    "             CAST(SUBSTRING_INDEX(SUBSTRING_INDEX(fight_stats.`TOTAL STR.`, ' of ', -1), '%', 1) AS INT)) / 2) AS AVG_TOTAL_STR,\n",
    "    AVG(CAST(SPLIT(fight_stats.Round, ' ')[1] AS INT)) AS AVG_ROUND,\n",
    "    AVG(\n",
    "            CASE \n",
    "                WHEN fight_stats.CTRL LIKE '%:%' THEN\n",
    "                    CAST(SPLIT(fight_stats.CTRL, ':')[0] AS INT) * 60 + CAST(SPLIT(fight_stats.CTRL, ':')[1] AS INT)\n",
    "                ELSE \n",
    "                    CAST(SPLIT(fight_stats.CTRL, ' ')[0] AS INT)\n",
    "            END\n",
    "        ) AS AVG_CTRL_SECONDS\n",
    "    FROM \n",
    "        fighter_info JOIN fight_stats ON fighter_info.FIGHTER = fight_stats.FIGHTER \n",
    "    GROUP BY \n",
    "        fighter_info.FIGHTER, fighter_info.Height, fighter_info.Weight, fighter_info.Reach\n",
    "\"\"\")\n",
    "\n",
    "updated_fighter_info.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T11:36:20.456355Z",
     "start_time": "2024-05-08T11:36:18.750441Z"
    }
   },
   "id": "56c1c8752dd9c26f",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adding result table to the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28dd51a2945a510e"
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import split, expr\n",
    "\n",
    "# Extract fighters' names from the \"BOUT\" column\n",
    "fight_results = df_operational_to_calc_result.withColumn(\"fighter1\", split(df_operational_to_calc_result[\"BOUT\"], \" vs. \")[0]) \\\n",
    "    .withColumn(\"fighter2\", split(df_operational_to_calc_result[\"BOUT\"], \" vs. \")[1])\n",
    "\n",
    "#spark sql query select outcome, fighter1 and fighter 2\n",
    "fight_results.createOrReplaceTempView(\"fight_results\")\n",
    "fight_results = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        fighter1,\n",
    "        fighter2,\n",
    "        outcome\n",
    "        from fight_results\n",
    "\"\"\")\n",
    "#fighter one gets outcome 1, fighter 2 gets outcome 0 when outcome is outcome1/outcome2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T11:37:16.480546Z",
     "start_time": "2024-05-08T11:37:16.448545Z"
    }
   },
   "id": "115d4f24b764921b",
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# add column accuracy with value 0\n",
    "fight_results.write.csv('../processed_data/fight_results.csv', header=True, mode='overwrite')\n",
    "fight_results = fight_results.toPandas()\n",
    "fight_results['Accuracy'] = 1\n",
    "df = spark.createDataFrame(fight_results)\n",
    "df.createOrReplaceTempView(\"fightresults\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T11:37:20.487211Z",
     "start_time": "2024-05-08T11:37:19.641341Z"
    }
   },
   "id": "7c5780e717fab3bf",
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.createOrReplaceTempView(\"fight_results\")\n",
    "\n",
    "win_loss_percentage = spark.sql(\"\"\"\n",
    "    SELECT fighter, \n",
    "       SUM(wins) AS total_wins, \n",
    "       SUM(losses) AS total_losses, \n",
    "       SUM(draws) AS total_draws\n",
    "FROM (\n",
    "    SELECT fighter1 AS fighter,\n",
    "           CASE \n",
    "               WHEN outcome = 'W/L' THEN 1\n",
    "               WHEN outcome = 'L/W' THEN 0\n",
    "               ELSE 0\n",
    "           END AS wins,\n",
    "           CASE \n",
    "               WHEN outcome = 'W/L' THEN 0\n",
    "               WHEN outcome = 'L/W' THEN 1\n",
    "               ELSE 0\n",
    "           END AS losses,\n",
    "           CASE \n",
    "               WHEN outcome = 'D' THEN 1\n",
    "               ELSE 0\n",
    "           END AS draws\n",
    "    FROM fight_results\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    SELECT fighter2 AS fighter,\n",
    "           CASE \n",
    "               WHEN outcome = 'W/L' THEN 0\n",
    "               WHEN outcome = 'L/W' THEN 1\n",
    "               ELSE 0\n",
    "           END AS wins,\n",
    "           CASE \n",
    "               WHEN outcome = 'W/L' THEN 1\n",
    "               WHEN outcome = 'L/W' THEN 0\n",
    "               ELSE 0\n",
    "           END AS losses,\n",
    "           CASE \n",
    "               WHEN outcome = 'D' THEN 1\n",
    "               ELSE 0\n",
    "           END AS draws\n",
    "    FROM fight_results\n",
    ") AS all_results\n",
    "GROUP BY fighter;\n",
    "\"\"\")\n",
    "\n",
    "win_loss_percentage = win_loss_percentage.withColumn(\"Win_Percentage\", expr(\"total_wins / (total_wins + total_losses + total_draws) * 100\"))\n",
    "# rename the column fighter to fighter_name\n",
    "win_loss_percentage = win_loss_percentage.withColumnRenamed(\"fighter\", \"fighter_name\")\n",
    "win_loss_percentage.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T11:37:39.546991Z",
     "start_time": "2024-05-08T11:37:23.139299Z"
    }
   },
   "id": "698b363ebcf41136",
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Joining the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77690d54c1df415"
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Joining the two tables on the fighter and FIGHTER columns\n",
    "joined_df = updated_fighter_info.alias(\"a\").join(win_loss_percentage.alias(\"b\"),\n",
    "                                      expr(\"trim(a.fighter) = trim(b.fighter_name)\"))\n",
    "\n",
    "# Showing the joined DataFrame\n",
    "joined_df.show(100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T11:37:55.214143Z",
     "start_time": "2024-05-08T11:37:39.547992Z"
    }
   },
   "id": "fa0d8c485a72bb22",
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Register the joined DataFrame as a temporary view\n",
    "joined_df.createOrReplaceTempView(\"joined_df\")\n",
    "\n",
    "# Convert the height from feet and inches to centimeters using SQL\n",
    "converted_height_df = spark.sql(\"\"\"\n",
    "    SELECT *,\n",
    "           (CAST(SPLIT(Height, \"'\")[0] AS INT) * 30.48 +\n",
    "            CAST(REGEXP_REPLACE(SPLIT(SUBSTR(Height, INSTR(Height, \"'\") + 1), '\"')[0], '\\\\\\\\\"', '') AS INT) * 2.54) AS Height_CM,\n",
    "           CAST(REGEXP_REPLACE(Reach, '[\"]', '') AS INT) AS Reach_Conv,\n",
    "           CAST(REGEXP_REPLACE(Weight, ' lbs.', '') AS INT) * 0.453592 AS Weight_KG\n",
    "    FROM joined_df\n",
    "\"\"\")\n",
    "\n",
    "# Show the DataFrame with the converted height\n",
    "# remove the Height column\n",
    "converted_height_df = converted_height_df.drop(\"Height\", \"Weight\", \"Reach\")\n",
    "converted_height_df = converted_height_df.drop(\"fighter_name\")\n",
    "converted_height_df.show(1000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T11:38:11.129348Z",
     "start_time": "2024-05-08T11:37:55.215142Z"
    }
   },
   "id": "91dac238d3fde8e4",
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "# convert converted_height_df to pandas dataframe\n",
    "converted_height_df_pd = converted_height_df.toPandas()\n",
    "numerical_cols = ['AVG_KD', 'AVG_SUB_ATT', 'AVG_TD_Percentage', 'AVG_Significant_Strike_Percentage', 'AVG_TOTAL_STR', 'AVG_ROUND', 'AVG_CTRL_SECONDS', 'total_wins', 'total_losses', 'total_draws', 'Win_Percentage', 'Height_CM', 'Reach_Conv', 'Weight_KG']\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "converted_height_df_pd_imputed = imputer.fit_transform(converted_height_df_pd[numerical_cols])\n",
    "converted_height_df_pd_imputed"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T11:38:27.579820Z",
     "start_time": "2024-05-08T11:38:11.131349Z"
    }
   },
   "id": "33178bd8d8e795f2",
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# put the imputed data back into the original spark dataframe converted_height_df\n",
    "converted_height_df_pd[numerical_cols] = converted_height_df_pd_imputed\n",
    "converted_height_df = spark.createDataFrame(converted_height_df_pd)\n",
    "converted_height_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T11:38:28.145978Z",
     "start_time": "2024-05-08T11:38:27.580822Z"
    }
   },
   "id": "356ca612121085a5",
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"FIGHTER\", outputCol=\"fighter_index\")\n",
    "converted_height_df = indexer.fit(converted_height_df).transform(converted_height_df)\n",
    "converted_height_df.show(1000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T11:38:41.403387Z",
     "start_time": "2024-05-08T11:38:28.146981Z"
    }
   },
   "id": "60cd852bcb115f01",
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "converted_height_df.write.csv('../processed_data/fighter_details.csv', header=True, mode='overwrite')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T14:34:08.152979200Z",
     "start_time": "2024-04-25T14:33:58.047137200Z"
    }
   },
   "id": "795e8d8c1fe23710",
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Joining the result with the fighters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a05a12e9b25b8a1"
  },
  {
   "cell_type": "code",
   "source": [
    "fight_results = spark.read.csv(\"../processed_data/fight_results.csv\", header=True)\n",
    "fight_results.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T14:34:08.334143900Z",
     "start_time": "2024-04-25T14:34:08.153976900Z"
    }
   },
   "id": "529d629a9d2da264",
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "fighter_details = spark.read.csv(\"../processed_data/fighter_details.csv\", header=True, inferSchema=True)\n",
    "fighter_details.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T14:34:08.859150400Z",
     "start_time": "2024-04-25T14:34:08.328154200Z"
    }
   },
   "id": "f0e7a7409145308c",
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "fight_results.createOrReplaceTempView(\"match_results\")\n",
    "fighter_details.createOrReplaceTempView(\"fighters\")\n",
    "\n",
    "total_df = spark.sql(\"\"\"SELECT\n",
    "    mr.fighter1,\n",
    "    fr1.fighter_index as fighter1_index,\n",
    "    fr1.Height_CM AS height1,\n",
    "    fr1.Weight_KG AS weight_kg1,\n",
    "    fr1.AVG_KD AS avg_kd1,\n",
    "    fr1.AVG_SUB_ATT AS avg_sub_att1,\n",
    "    fr1.AVG_TD_Percentage AS avg_td_percentage1,\n",
    "    fr1.AVG_Significant_Strike_Percentage AS avg_significant_strike_percentage1,\n",
    "    fr1.AVG_TOTAL_STR AS avg_total_str1,\n",
    "    fr1.AVG_ROUND AS avg_round1,\n",
    "    fr1.AVG_CTRL_SECONDS AS avg_ctrl_seconds1,\n",
    "    fr1.total_wins AS total_wins1,\n",
    "    fr1.total_losses AS total_losses1,\n",
    "    fr1.total_draws AS total_draws1,\n",
    "    fr1.Win_Percentage AS win_percentage1,\n",
    "    fr1.Reach_Conv AS reach_conv1,\n",
    "    mr.fighter2,\n",
    "    fr2.fighter_index as fighter2_index,\n",
    "    fr2.Height_CM AS height2,\n",
    "    fr2.Weight_KG AS weight_kg2,\n",
    "    fr2.AVG_KD AS avg_kd2,\n",
    "    fr2.AVG_SUB_ATT AS avg_sub_att2,\n",
    "    fr2.AVG_TD_Percentage AS avg_td_percentage2,\n",
    "    fr2.AVG_Significant_Strike_Percentage AS avg_significant_strike_percentage2,\n",
    "    fr2.AVG_TOTAL_STR AS avg_total_str2,\n",
    "    fr2.AVG_ROUND AS avg_round2,\n",
    "    fr2.AVG_CTRL_SECONDS AS avg_ctrl_seconds2,\n",
    "    fr2.total_wins AS total_wins2,\n",
    "    fr2.total_losses AS total_losses2,\n",
    "    fr2.total_draws AS total_draws2,\n",
    "    fr2.Win_Percentage AS win_percentage2,\n",
    "    fr2.Reach_Conv AS reach_conv2,\n",
    "    mr.outcome\n",
    "FROM\n",
    "    match_results mr\n",
    "JOIN\n",
    "    fighters fr1 ON mr.fighter1 = fr1.fighter\n",
    "JOIN\n",
    "    fighters fr2 ON mr.fighter2 = fr2.fighter;\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "total_df.write.csv(\"../processed_data/fight_total.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "\n",
    "total_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T14:34:10.164998Z",
     "start_time": "2024-04-25T14:34:08.859150400Z"
    }
   },
   "id": "be184054b7511baf",
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### upcoming events and fights"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffe32fe2de7709fa"
  },
  {
   "cell_type": "code",
   "source": [
    "upcoming_events_df = spark.read.csv(\"../scraped_data/upcoming_event_details.csv\", header=True)\n",
    "\n",
    "upcoming_events_df.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T14:34:10.365633Z",
     "start_time": "2024-04-25T14:34:10.164998Z"
    }
   },
   "id": "c71b65e1d168a26",
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "upcoming_fights_df = spark.read.csv(\"../scraped_data/upcoming_fight_details.csv\", header=True)\n",
    "\n",
    "upcoming_fights_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T14:34:10.537756900Z",
     "start_time": "2024-04-25T14:34:10.352639500Z"
    }
   },
   "id": "284b830102ced3f3",
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "now we will combine the data from the upcoming fights and the fighters and their event details, so we want fight, fighter1, fighter2, event details. We get the fighters from the bout"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "190cca9e8e5f6081"
  },
  {
   "cell_type": "code",
   "source": [
    "upcoming_events_df.createOrReplaceTempView(\"upcoming_events\")\n",
    "upcoming_fights_df.createOrReplaceTempView(\"upcoming_fights\")\n",
    "\n",
    "upcoming_events_fights_df = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "    uf.event,\n",
    "    ue.date AS event_date,\n",
    "    ue.location AS event_location,\n",
    "    SPLIT(uf.bout, ' vs. ')[0] AS fighter1,\n",
    "    SPLIT(uf.bout, ' vs. ')[1] AS fighter2\n",
    "FROM\n",
    "    upcoming_fights uf\n",
    "JOIN\n",
    "    upcoming_events ue\n",
    "ON\n",
    "    uf.event = ue.event\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "upcoming_events_fights_df.show()\n",
    "upcoming_events_fights_df.write.csv(\"../processed_data/upcoming_events_fights.csv\", header=True, mode=\"overwrite\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T14:34:10.954944600Z",
     "start_time": "2024-04-25T14:34:10.524767100Z"
    }
   },
   "id": "f6b2e97641f907c5",
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "upcoming_events_fights_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T14:34:11.122081600Z",
     "start_time": "2024-04-25T14:34:10.929950200Z"
    }
   },
   "id": "26c9a55d78266846",
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# upcomingevents and fightresults\n",
    "upcoming_events_fights_df.createOrReplaceTempView(\"upcomingevents\")\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T14:34:12.008569900Z",
     "start_time": "2024-04-25T14:34:11.103536500Z"
    }
   },
   "id": "4c200ae7117b4f66",
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# combine both spark dataframes on column fighter1 and fighter2\n",
    "combined = spark.sql(\"\"\n",
    "                     \"SELECT ue.event \"\n",
    "                     \"FROM upcomingevents ue \"\n",
    "                     \"JOIN fight_results fr \"\n",
    "                     \"ON fr.fighter1 == ue.fighter1 \"\n",
    "                     \"AND fr.fighter2 == ue.fighter2\")\n",
    "combined.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T14:34:21.909190900Z",
     "start_time": "2024-04-25T14:34:11.993555Z"
    }
   },
   "id": "d9976056d4b0246",
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# upcoming_events = spark.read.csv(\"processed_data/upcoming_events.csv\", header=True)\n",
    "# \n",
    "# #change event_date from June 22, 2024 to 2024-06-22\n",
    "# \n",
    "# upcoming_events = upcoming_events.withColumn(\"event_date\", expr(\"date_format(to_date(event_date, 'MMMM dd, yyyy'), 'yyyy-MM-dd')\"))\n",
    "# \n",
    "# upcoming_events.show()\n",
    "# \n",
    "# upcoming_events.toPandas().to_csv(\"processed_data/upcoming_events.csv\", index=False, sep=\";\")\n",
    "# \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T14:34:21.911170600Z",
     "start_time": "2024-04-25T14:34:21.897180200Z"
    }
   },
   "id": "8ed4faed80fbb0c6",
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "fights = spark.read.csv(\"../processed_data/final_table_fights.csv\", header=True)\n",
    "\n",
    "\n",
    "\n",
    "fights = fights.toPandas()\n",
    "\n",
    "fights.to_csv(\"../processed_data/final_table_fights.csv\", index=False, sep=\";\")\n",
    "\n",
    "fights.head()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T14:34:22.087324200Z",
     "start_time": "2024-04-25T14:34:21.912188200Z"
    }
   },
   "id": "41b68f703d2cd79b",
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "fighter_details = spark.read.csv(\"../processed_data/fighter_details.csv\", header=True)\n",
    "\n",
    "fighter_details = fighter_details.toPandas()\n",
    "\n",
    "fighter_details.to_csv(\"../processed_data/fighter_details_final.csv\", index=False, sep=\";\")\n",
    "\n",
    "fighter_details.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T14:34:22.397955300Z",
     "start_time": "2024-04-25T14:34:22.085314900Z"
    }
   },
   "id": "3d01de76360642ae",
   "execution_count": 26,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
